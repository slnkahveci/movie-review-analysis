{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Sentiment Analysis - Complete Interface\n",
    "\n",
    "This notebook provides an interface for all project routines:\n",
    "1. **Data Preprocessing**\n",
    "2. **Statistics & Visualization**\n",
    "3. **N-gram Models** (training & generation)\n",
    "4. **RNN Models** (LSTM/GRU training & generation)\n",
    "5. **Transformer Models** (training & evaluation)\n",
    "\n",
    "---\n",
    "\n",
    "## Overall Project Pipeline\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[IMDB Dataset CSV] --> B[TextPreprocessor]\n",
    "    B --> C{Tokenization Type}\n",
    "    C -->|Word| D[Word Tokenization]\n",
    "    C -->|Subword| E[Subword Tokenization]\n",
    "    \n",
    "    D --> F[Statistics & Visualization]\n",
    "    D --> G[N-gram Models]\n",
    "    D --> H[RNN Models LSTM/GRU]\n",
    "    E --> I[Transformer Models]\n",
    "    \n",
    "    F --> J[Word Clouds & Scattertext]\n",
    "    G --> K[Text Generation & Perplexity]\n",
    "    H --> L[Text Generation & Perplexity]\n",
    "    I --> M[Sentiment Classification]\n",
    "    \n",
    "    M --> N[Evaluation Pipeline]\n",
    "    N --> O[Metric-based]\n",
    "    N --> P[Human Evaluation]\n",
    "    N --> Q[LLM-as-a-Judge]\n",
    "    \n",
    "    O --> R[Results & Comparison]\n",
    "    P --> R\n",
    "    Q --> R\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style B fill:#fff4e1\n",
    "    style J fill:#e8f5e9\n",
    "    style K fill:#e8f5e9\n",
    "    style L fill:#e8f5e9\n",
    "    style M fill:#e8f5e9\n",
    "    style R fill:#f3e5f5\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Configuration\n",
    "\n",
    "### Project Architecture\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    subgraph Data Layer\n",
    "        A[IMDB CSV] --> B[TextPreprocessor]\n",
    "        B --> C[Train/Val/Test Splits]\n",
    "    end\n",
    "    \n",
    "    subgraph Model Layer\n",
    "        C --> D[NGramModel]\n",
    "        C --> E[RNN LSTM/GRU]\n",
    "        C --> F[Transformer ALBERT]\n",
    "    end\n",
    "    \n",
    "    subgraph Evaluation Layer\n",
    "        D --> G[Perplexity]\n",
    "        E --> G\n",
    "        F --> H[Multi-Perspective Eval]\n",
    "        H --> I[Metrics]\n",
    "        H --> J[Human]\n",
    "        H --> K[LLM Judge]\n",
    "    end\n",
    "    \n",
    "    subgraph Output Layer\n",
    "        G --> L[out/ directory]\n",
    "        I --> L\n",
    "        J --> L\n",
    "        K --> L\n",
    "    end\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style D fill:#fff4e1\n",
    "    style E fill:#fff4e1\n",
    "    style F fill:#fff4e1\n",
    "    style L fill:#e8f5e9\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from src.data.preprocessing import TextPreprocessor, SENTIMENT_TO_ID\n",
    "from src.data.stats import IMDBDataStats\n",
    "from src.models.ngram import NGramModel\n",
    "from src.models.nn import RNN, RNNDataModule, get_device\n",
    "from src.models.transformer import finetune_minilm, TransformerDataset\n",
    "from src.eval.eval_transformer import evaluate_transformer\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def split_dataset_indices(\n",
    "    dataset_length: int,\n",
    "    train_ratio: float = 0.8,\n",
    "    val_ratio: float = 0.1,\n",
    "    test_ratio: float = 0.1,\n",
    "    n_selected_test: int = 100,\n",
    "    seed: int = 42,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Split dataset indices into train, eval, test, and a selected test subset.\n",
    "\n",
    "    Returns:\n",
    "        train_idx, val_idx, test_idx, selected_test_idx\n",
    "    \"\"\"\n",
    "    assert (\n",
    "        abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6\n",
    "    ), \"Ratios must sum to 1\"\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    indices = rng.permutation(dataset_length)\n",
    "\n",
    "    train_end = int(dataset_length * train_ratio)\n",
    "    eval_end = train_end + int(dataset_length * val_ratio)\n",
    "\n",
    "    train_idx = indices[:train_end]\n",
    "    val_idx = indices[train_end:eval_end]\n",
    "    test_idx = indices[eval_end:]\n",
    "\n",
    "    # Select subset from test indices\n",
    "    n_selected = min(n_selected_test, len(test_idx))\n",
    "    selected_test_idx = rng.choice(test_idx, size=n_selected, replace=False)\n",
    "\n",
    "    return train_idx, val_idx, test_idx, selected_test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_PATH = \"dataset/imdb-dataset.csv\"\n",
    "SAMPLE_SIZE = 1000  # Reduce for faster experimentation\n",
    "DEVICE = get_device()\n",
    "import sklearn\n",
    "\n",
    "\n",
    "train_idx, val_idx, test_idx, selected_test_idx = split_dataset_indices(SAMPLE_SIZE)\n",
    "# save selected_test_idx to a csv file\n",
    "selected_test_df = pd.read_csv(DATA_PATH).iloc[selected_test_idx]\n",
    "selected_test_df.to_csv(f\"dataset/imdb-test-subsample-100_{SAMPLE_SIZE}.csv\", index=False)\n",
    "selected_test_path = f\"dataset/imdb-test-subsample-100_{SAMPLE_SIZE}.csv\"\n",
    "\n",
    "# selected_test_idx is a subset of test_idx\n",
    "assert set(selected_test_idx).issubset(set(test_idx))\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Sample size: {SAMPLE_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Data Preprocessing\n",
    "\n",
    "Load and preprocess the IMDB dataset with different tokenization strategies.\n",
    "\n",
    "### Preprocessing Pipeline\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Raw CSV] --> B[Load & Sample]\n",
    "    B --> C[Clean HTML Tags]\n",
    "    C --> D[Lowercase]\n",
    "    D --> E[Expand Contractions]\n",
    "    E --> F[Remove Punctuation]\n",
    "    F --> G{Tokenization}\n",
    "    G -->|Word| H[NLTK word_tokenize]\n",
    "    G -->|Subword| I[HuggingFace Tokenizer]\n",
    "    H --> J[Word Lists]\n",
    "    I --> K[Token IDs]\n",
    "    J --> L[Train/Val/Test Split]\n",
    "    K --> L\n",
    "    L --> M[Ready for Models]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style G fill:#fff4e1\n",
    "    style M fill:#e8f5e9\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word-based tokenization (for stats, ngrams, RNNs)\n",
    "preprocessor_word = TextPreprocessor(\n",
    "    DATA_PATH,\n",
    "    sample_size=SAMPLE_SIZE,\n",
    "    tokenizer_type=\"word\"\n",
    ")\n",
    "\n",
    "df_word = preprocessor_word.load_data(remove_stopwords=True) # true for better stats\n",
    "train_df, val_df, test_df = preprocessor_word.get_splits(train_index=train_idx, val_index=val_idx, test_index=test_idx)\n",
    "\n",
    "print(\"\\nDataset loaded successfully!\")\n",
    "display(df_word.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Statistics & Visualization\n",
    "\n",
    "Compute statistics and visualize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics\n",
    "stats_computer = IMDBDataStats(df_word)\n",
    "stats = stats_computer.get_full_stats()\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nClass Distribution: {stats['class_distribution']}\")\n",
    "print(f\"\\nVocabulary Size: {stats['vocabulary_size']}\")\n",
    "print(f\"\\nAverage Word Count: {stats['average_word_count']}\")\n",
    "print(f\"\\nMost Frequent Words (Overall):\")\n",
    "for word, count in stats['most_frequent_words']['overall']:\n",
    "    print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize word clouds\n",
    "stats_computer.visualize_word_clouds(save_path=\"out/wordclouds.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate scattertext visualization (interactive HTML)\n",
    "stats_computer.visualize_scattertext(output_html=\"out/scattertext.html\")\n",
    "print(\"Open out/scattertext.html in your browser to view the interactive visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. N-gram Language Models\n",
    "\n",
    "Train bigram and trigram models, generate text, and compute perplexity.\n",
    "\n",
    "### N-gram Pipeline\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Tokenized Text] --> B[Build N-gram Counts]\n",
    "    B --> C{N-gram Type}\n",
    "    C -->|n=2| D[Bigram Model]\n",
    "    C -->|n=3| E[Trigram Model]\n",
    "    \n",
    "    D --> F[Laplace Smoothing]\n",
    "    E --> F\n",
    "    \n",
    "    F --> G[Probability Estimation]\n",
    "    G --> H[Text Generation]\n",
    "    G --> I[Perplexity Computation]\n",
    "    \n",
    "    H --> J[Sample Sentences]\n",
    "    I --> K[Model Quality Metric]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style F fill:#fff4e1\n",
    "    style J fill:#e8f5e9\n",
    "    style K fill:#e8f5e9\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new dataframe loaded with stopwords\n",
    "\n",
    "train_df = preprocessor_word.load_data(remove_stopwords=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Bigram Model\n",
    "print(\"Training Bigram Model...\")\n",
    "bigram_model = NGramModel(n=2, laplace_smoothing=True)\n",
    "bigram_model.train(train_df[\"_words\"])\n",
    "\n",
    "print(\"\\n--- Bigram Generation Examples ---\")\n",
    "for i in range(3):\n",
    "    print(f\"{i+1}. {bigram_model.generate_sentence(max_length=20)}\")\n",
    "\n",
    "print(\"\\nComputing perplexity...\")\n",
    "bigram_perplexity = bigram_model.compute_perplexity(test_df[\"_words\"])\n",
    "print(f\"Bigram Test Perplexity: {bigram_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Trigram Model\n",
    "print(\"Training Trigram Model...\")\n",
    "trigram_model = NGramModel(n=3, laplace_smoothing=True)\n",
    "trigram_model.train(train_df[\"_words\"])\n",
    "\n",
    "print(\"\\n--- Trigram Generation Examples ---\")\n",
    "for i in range(3):\n",
    "    print(f\"{i+1}. {trigram_model.generate_sentence(max_length=20)}\")\n",
    "\n",
    "print(\"\\nComputing perplexity...\")\n",
    "trigram_perplexity = trigram_model.compute_perplexity(test_df[\"_words\"])\n",
    "print(f\"Trigram Test Perplexity: {trigram_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. RNN Models (LSTM/GRU)\n",
    "\n",
    "Train recurrent neural networks for language modeling and text generation.\n",
    "\n",
    "### RNN Training Pipeline\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Word Tokens] --> B[Build Vocabulary]\n",
    "    B --> C[Token to ID Mapping]\n",
    "    C --> D[Create DataLoader]\n",
    "    D --> E[RNN Model]\n",
    "    \n",
    "    E --> F{RNN Type}\n",
    "    F -->|LSTM| G[LSTM Layers]\n",
    "    F -->|GRU| H[GRU Layers]\n",
    "    \n",
    "    G --> I[Embedding Layer]\n",
    "    H --> I\n",
    "    I --> J[Hidden States]\n",
    "    J --> K[Output Layer]\n",
    "    \n",
    "    K --> L[Cross-Entropy Loss]\n",
    "    L --> M[Backpropagation]\n",
    "    M --> N{Converged?}\n",
    "    N -->|No| E\n",
    "    N -->|Yes| O[Trained Model]\n",
    "    \n",
    "    O --> P[Text Generation]\n",
    "    O --> Q[Perplexity Evaluation]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style E fill:#fff4e1\n",
    "    style O fill:#e8f5e9\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "data_module = RNNDataModule()\n",
    "data_module.build_vocab(train_df[\"_words\"].tolist(), min_freq=5) # train_df with stopwords\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = data_module.get_dataloader(\n",
    "    df=train_df,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    max_seq_length=128,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "val_loader = data_module.get_dataloader(\n",
    "    df=val_df,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    max_seq_length=128,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "test_loader = data_module.get_dataloader(\n",
    "    df=test_df,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    max_seq_length=128,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "print(f\"Vocabulary size: {len(data_module.vocab)}\")\n",
    "print(f\"Train batches: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM Model\n",
    "print(\"Training LSTM Model...\")\n",
    "\n",
    "lstm_model = RNN(\n",
    "    vocab_size=len(data_module.vocab),\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=256,\n",
    "    rnn_type='LSTM',\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0)  # padding_idx=0\n",
    "optimizer = torch.optim.AdamW(lstm_model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2)\n",
    "\n",
    "lstm_model.train_loop(\n",
    "    train_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    num_epochs=3,  # Reduce for demo\n",
    "    accumulation_steps=4\n",
    ")\n",
    "\n",
    "# Save model\n",
    "lstm_model.save_model(\"out/lstm_imdb_model.pth\")\n",
    "print(\"Model saved to out/lstm_imdb_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with LSTM\n",
    "print(\"\\n--- LSTM Generation Examples ---\")\n",
    "start_token_id = data_module.vocab.get(\"<s>\", 2)\n",
    "\n",
    "for i in range(3):\n",
    "    generated_sequence = lstm_model.generate(\n",
    "        start_token=start_token_id,\n",
    "        max_length=30,\n",
    "        temperature=0.8\n",
    "    )\n",
    "    generated_text = data_module.decode_sequence(generated_sequence)\n",
    "    print(f\"{i+1}. {generated_text}\")\n",
    "\n",
    "# Compute perplexity\n",
    "lstm_perplexity = lstm_model.compute_perplexity(test_loader)\n",
    "print(f\"\\nLSTM Test Perplexity: {lstm_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GRU Model (optional)\n",
    "print(\"Training GRU Model...\")\n",
    "\n",
    "gru_model = RNN(\n",
    "    vocab_size=len(data_module.vocab),\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=256,\n",
    "    rnn_type='GRU',\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(gru_model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2)\n",
    "\n",
    "gru_model.train_loop(\n",
    "    train_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    num_epochs=3,\n",
    "    accumulation_steps=4\n",
    ")\n",
    "\n",
    "gru_model.save_model(\"out/gru_imdb_model.pth\")\n",
    "print(\"Model saved to out/gru_imdb_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with GRU\n",
    "print(\"\\n--- GRU Generation Examples ---\")\n",
    "\n",
    "for i in range(3):\n",
    "    generated_sequence = gru_model.generate(\n",
    "        start_token=start_token_id,\n",
    "        max_length=30,\n",
    "        temperature=0.8\n",
    "    )\n",
    "    generated_text = data_module.decode_sequence(generated_sequence)\n",
    "    print(f\"{i+1}. {generated_text}\")\n",
    "\n",
    "# Compute perplexity\n",
    "gru_perplexity = gru_model.compute_perplexity(test_loader)\n",
    "print(f\"\\nGRU Test Perplexity: {gru_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Transformer Models (Fine-tuning)\n",
    "\n",
    "Fine-tune pre-trained transformers (ALBERT) on sentiment classification.\n",
    "\n",
    "### Transformer Fine-tuning Pipeline\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[IMDB Dataset] --> B[Subword Tokenization]\n",
    "    B --> C{Tokenizer Type}\n",
    "    C -->|WordPiece| D[BERT/MiniLM Tokenizer]\n",
    "    C -->|BPE| E[RoBERTa Tokenizer]\n",
    "    C -->|Unigram| F[ALBERT Tokenizer]\n",
    "    \n",
    "    D --> G[Token IDs + Attention Masks]\n",
    "    E --> G\n",
    "    F --> G\n",
    "    \n",
    "    G --> H[Pre-trained Model]\n",
    "    H --> I[Add Classification Head]\n",
    "    I --> J[Fine-tuning]\n",
    "    \n",
    "    J --> K[Training Loop]\n",
    "    K --> L[Gradient Accumulation]\n",
    "    L --> M[AdamW Optimizer]\n",
    "    M --> N{Epoch Complete?}\n",
    "    N -->|No| K\n",
    "    N -->|Yes| O[Save Model]\n",
    "    \n",
    "    O --> P[Evaluation]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style C fill:#fff4e1\n",
    "    style O fill:#e8f5e9\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Fine-tuning Transformer...\")\n",
    "\n",
    "transformer_outputs = finetune_minilm(\n",
    "    data_path=DATA_PATH,\n",
    "    sample_size=SAMPLE_SIZE,\n",
    "    output_dir=\"out/minilm_imdb_model\",\n",
    "    tokenization=\"all\",  # Options: \"wordpiece\", \"bpe\", \"unigram\", \"all\"\n",
    "    epochs=1,\n",
    "    train_index=train_idx,\n",
    "    val_index=val_idx,\n",
    "    test_index=test_idx\n",
    ")\n",
    "\n",
    "print(\"\\nTransformer training complete!\")\n",
    "print(f\"Model saved to out/minilm_imdb_model\")\n",
    "print(transformer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Transformer Evaluation\n",
    "\n",
    "Evaluate the fine-tuned transformer with:\n",
    "- Standard metrics (Accuracy, F1, Confusion Matrix, Perplexity)\n",
    "- LLM-as-a-judge (optional, requires API key)\n",
    "\n",
    "### Three-Part Evaluation Pipeline\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Trained Model] --> B[Random 100 Test Samples]\n",
    "    \n",
    "    B --> C[Part 1: Metric-based]\n",
    "    B --> D[Part 2: Human Evaluation]\n",
    "    B --> E[Part 3: LLM-as-a-Judge]\n",
    "    \n",
    "    C --> F[Accuracy]\n",
    "    C --> G[F1 Score]\n",
    "    C --> H[Perplexity]\n",
    "    C --> I[Confusion Matrix]\n",
    "    \n",
    "    D --> J[CSV Export]\n",
    "    J --> K[Manual Annotation]\n",
    "    K --> L[Human Ratings]\n",
    "    \n",
    "    E --> M[Llama 3.1 8B]\n",
    "    M --> N[LLM Judgments]\n",
    "    \n",
    "    F --> O[Agreement Analysis]\n",
    "    G --> O\n",
    "    H --> O\n",
    "    I --> O\n",
    "    L --> O\n",
    "    N --> O\n",
    "    \n",
    "    O --> P[Compare All Evaluations]\n",
    "    P --> Q[Disagreement Examples]\n",
    "    P --> R[Final Report]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style C fill:#fff4e1\n",
    "    style D fill:#fff4e1\n",
    "    style E fill:#fff4e1\n",
    "    style R fill:#e8f5e9\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Google Gemini API key (optional, for LLM judge)\n",
    "import os\n",
    "# Run evaluation\n",
    "# create subsample from test set\n",
    "print(selected_test_idx)\n",
    "\n",
    "\n",
    "for tokenization in [\"wordpiece\"]:\n",
    "    model_dir = f\"out/minilm_imdb_model_{tokenization}\"\n",
    "    print(f\"\\nEvaluating Transformer model ({tokenization})...\")\n",
    "    evaluate_transformer(\n",
    "        model_dir=model_dir,\n",
    "        tokenizer_type=tokenization,\n",
    "        train_index=train_idx,\n",
    "        eval_index=val_idx,\n",
    "        test_index=selected_test_idx,\n",
    "        test_path=selected_test_path,\n",
    "        data_path=DATA_PATH,\n",
    "        sample_size=SAMPLE_SIZE,\n",
    "        skip_human_eval=False,  # Set to False to generate CSV for human annotation\n",
    "        skip_llm_judge=False,  # Set to False to use LLM judge\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Model Comparison Summary\n",
    "\n",
    "Compare all models' perplexity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "summary_data = {\n",
    "    'Model': ['Bigram', 'Trigram', 'LSTM', 'GRU'],\n",
    "    'Test Perplexity': [\n",
    "        bigram_perplexity if 'bigram_perplexity' in locals() else 'N/A',\n",
    "        trigram_perplexity if 'trigram_perplexity' in locals() else 'N/A',\n",
    "        lstm_perplexity if 'lstm_perplexity' in locals() else 'N/A',\n",
    "        gru_perplexity if 'gru_perplexity' in locals() else 'N/A'\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot perplexity comparison\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "if all(k in locals() for k in ['bigram_perplexity', 'trigram_perplexity', 'lstm_perplexity', 'gru_perplexity']):\n",
    "    models = ['Bigram', 'Trigram', 'LSTM', 'GRU']\n",
    "    perplexities = [bigram_perplexity, trigram_perplexity, lstm_perplexity, gru_perplexity]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(models, perplexities, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
    "    plt.xlabel('Model', fontsize=12)\n",
    "    plt.ylabel('Test Perplexity', fontsize=12)\n",
    "    plt.title('Model Comparison: Test Perplexity', fontsize=14)\n",
    "    plt.yscale('log')  # Log scale for better visualization\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(perplexities):\n",
    "        plt.text(i, v, f'{v:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('out/model_comparison.png', dpi=150)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Run all model training cells to generate comparison plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Notes\n",
    "\n",
    "- **Adjust `SAMPLE_SIZE`** in the configuration cell to control dataset size\n",
    "- **Training times** vary by model complexity and hardware\n",
    "- **Transformer evaluation** supports LLM-as-a-judge with free OpenRouter API\n",
    "- All models are saved to disk and can be reloaded\n",
    "\n",
    "### File Outputs (saved in `out/` directory)\n",
    "- `out/wordclouds.png` - Word cloud visualizations\n",
    "- `out/scattertext.html` - Interactive term frequency visualization\n",
    "- `out/lstm_imdb_model.pth` - Trained LSTM model\n",
    "- `out/gru_imdb_model.pth` - Trained GRU model\n",
    "- `out/minilm_imdb_model/` - Fine-tuned transformer model\n",
    "- `out/model_comparison.png` - Perplexity comparison chart\n",
    "- `out/evaluation_results.csv` - Evaluation results for manual annotation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
