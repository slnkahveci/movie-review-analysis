{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/movie-review-analysis\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Sentiment Analysis - Complete Interface\n",
    "\n",
    "This notebook provides an interface for all project routines:\n",
    "1. **Data Preprocessing**\n",
    "2. **Statistics & Visualization**\n",
    "3. **N-gram Models** (training & generation)\n",
    "4. **RNN Models** (LSTM/GRU training & generation)\n",
    "5. **Transformer Models** (training & evaluation)\n",
    "\n",
    "---\n",
    "\n",
    "## Overall Project Pipeline\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[IMDB Dataset CSV] --> B[TextPreprocessor]\n",
    "    B --> C{Tokenization Type}\n",
    "    C -->|Word| D[Word Tokenization]\n",
    "    C -->|Subword| E[Subword Tokenization]\n",
    "    \n",
    "    D --> F[Statistics & Visualization]\n",
    "    D --> G[N-gram Models]\n",
    "    D --> H[RNN Models LSTM/GRU]\n",
    "    E --> I[Transformer Models]\n",
    "    \n",
    "    F --> J[Word Clouds & Scattertext]\n",
    "    G --> K[Text Generation & Perplexity]\n",
    "    H --> L[Text Generation & Perplexity]\n",
    "    I --> M[Sentiment Classification]\n",
    "    \n",
    "    M --> N[Evaluation Pipeline]\n",
    "    N --> O[Metric-based]\n",
    "    N --> P[Human Evaluation]\n",
    "    N --> Q[LLM-as-a-Judge]\n",
    "    \n",
    "    O --> R[Results & Comparison]\n",
    "    P --> R\n",
    "    Q --> R\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style B fill:#fff4e1\n",
    "    style J fill:#e8f5e9\n",
    "    style K fill:#e8f5e9\n",
    "    style L fill:#e8f5e9\n",
    "    style M fill:#e8f5e9\n",
    "    style R fill:#f3e5f5\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Configuration\n",
    "\n",
    "### Project Architecture\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    subgraph Data Layer\n",
    "        A[IMDB CSV] --> B[TextPreprocessor]\n",
    "        B --> C[Train/Val/Test Splits]\n",
    "    end\n",
    "    \n",
    "    subgraph Model Layer\n",
    "        C --> D[NGramModel]\n",
    "        C --> E[RNN LSTM/GRU]\n",
    "        C --> F[Transformer ALBERT]\n",
    "    end\n",
    "    \n",
    "    subgraph Evaluation Layer\n",
    "        D --> G[Perplexity]\n",
    "        E --> G\n",
    "        F --> H[Multi-Perspective Eval]\n",
    "        H --> I[Metrics]\n",
    "        H --> J[Human]\n",
    "        H --> K[LLM Judge]\n",
    "    end\n",
    "    \n",
    "    subgraph Output Layer\n",
    "        G --> L[out/ directory]\n",
    "        I --> L\n",
    "        J --> L\n",
    "        K --> L\n",
    "    end\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style D fill:#fff4e1\n",
    "    style E fill:#fff4e1\n",
    "    style F fill:#fff4e1\n",
    "    style L fill:#e8f5e9\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/selin/Library/Python/3.9/lib/python/site-packages/google/api_core/_python_version_support.py:246: FutureWarning: You are using a non-supported Python version (3.9.20). Google will not post any further updates to google.api_core supporting this Python version. Please upgrade to the latest Python version, or at least Python 3.10, and then update google.api_core.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/selin/Library/Python/3.9/lib/python/site-packages/google/auth/__init__.py:54: FutureWarning: \n",
      "    You are using a Python version 3.9 past its end of life. Google will update\n",
      "    google-auth with critical bug fixes on a best-effort basis, but not\n",
      "    with any other fixes or features. Please upgrade your Python version,\n",
      "    and then update google-auth.\n",
      "    \n",
      "  warnings.warn(eol_message.format(\"3.9\"), FutureWarning)\n",
      "/Users/selin/Library/Python/3.9/lib/python/site-packages/google/oauth2/__init__.py:40: FutureWarning: \n",
      "    You are using a Python version 3.9 past its end of life. Google will update\n",
      "    google-auth with critical bug fixes on a best-effort basis, but not\n",
      "    with any other fixes or features. Please upgrade your Python version,\n",
      "    and then update google-auth.\n",
      "    \n",
      "  warnings.warn(eol_message.format(\"3.9\"), FutureWarning)\n",
      "/Users/selin/Uni/Python/movie-review-analysis/src/eval/llm_judge.py:10: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  import google.generativeai as genai\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from src.data.preprocessing import TextPreprocessor, SENTIMENT_TO_ID\n",
    "from src.data.stats import IMDBDataStats\n",
    "from src.models.ngram import NGramModel\n",
    "from src.models.nn import RNN, RNNDataModule, get_device\n",
    "from src.models.transformer import finetune_minilm, TransformerDataset\n",
    "from src.eval.eval_transformer import evaluate_transformer\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def split_dataset_indices(\n",
    "    dataset_length: int,\n",
    "    train_ratio: float = 0.8,\n",
    "    val_ratio: float = 0.1,\n",
    "    test_ratio: float = 0.1,\n",
    "    n_selected_test: int = 100,\n",
    "    seed: int = 42,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Split dataset indices into train, eval, test, and a selected test subset.\n",
    "\n",
    "    Returns:\n",
    "        train_idx, val_idx, test_idx, selected_test_idx\n",
    "    \"\"\"\n",
    "    assert (\n",
    "        abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6\n",
    "    ), \"Ratios must sum to 1\"\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    indices = rng.permutation(dataset_length)\n",
    "\n",
    "    train_end = int(dataset_length * train_ratio)\n",
    "    eval_end = train_end + int(dataset_length * val_ratio)\n",
    "\n",
    "    train_idx = indices[:train_end]\n",
    "    val_idx = indices[train_end:eval_end]\n",
    "    test_idx = indices[eval_end:]\n",
    "\n",
    "    # Select subset from test indices\n",
    "    n_selected = min(n_selected_test, len(test_idx))\n",
    "    selected_test_idx = rng.choice(test_idx, size=n_selected, replace=False)\n",
    "\n",
    "    return train_idx, val_idx, test_idx, selected_test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Sample size: 1000\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DATA_PATH = \"dataset/imdb-dataset.csv\"\n",
    "SAMPLE_SIZE = 1000  # Reduce for faster experimentation\n",
    "DEVICE = get_device()\n",
    "import sklearn\n",
    "\n",
    "\n",
    "train_idx, val_idx, test_idx, selected_test_idx = split_dataset_indices(SAMPLE_SIZE)\n",
    "# save selected_test_idx to a csv file\n",
    "selected_test_df = pd.read_csv(DATA_PATH).iloc[selected_test_idx]\n",
    "selected_test_df.to_csv(f\"dataset/imdb-test-subsample-100_{SAMPLE_SIZE}.csv\", index=False)\n",
    "selected_test_path = f\"dataset/imdb-test-subsample-100_{SAMPLE_SIZE}.csv\"\n",
    "\n",
    "# selected_test_idx is a subset of test_idx\n",
    "assert set(selected_test_idx).issubset(set(test_idx))\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Sample size: {SAMPLE_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Data Preprocessing\n",
    "\n",
    "Load and preprocess the IMDB dataset with different tokenization strategies.\n",
    "\n",
    "### Preprocessing Pipeline\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Raw CSV] --> B[Load & Sample]\n",
    "    B --> C[Clean HTML Tags]\n",
    "    C --> D[Lowercase]\n",
    "    D --> E[Expand Contractions]\n",
    "    E --> F[Remove Punctuation]\n",
    "    F --> G{Tokenization}\n",
    "    G -->|Word| H[NLTK word_tokenize]\n",
    "    G -->|Subword| I[HuggingFace Tokenizer]\n",
    "    H --> J[Word Lists]\n",
    "    I --> K[Token IDs]\n",
    "    J --> L[Train/Val/Test Split]\n",
    "    K --> L\n",
    "    L --> M[Ready for Models]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style G fill:#fff4e1\n",
    "    style M fill:#e8f5e9\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from dataset/imdb-dataset.csv...\n",
      "Loaded 1000 reviews (positive: 476, negative: 524)\n",
      "\n",
      "Dataset loaded successfully!\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentiment",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "_words",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "_word_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "_char_len",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "34f0803d-3002-4757-93d9-788a77a5dcc0",
       "rows": [
        [
         "0",
         "really liked summerslam due look arena curtains look overall interesting reason anyways could one best summerslams ever wwf lex luger main event yokozuna time ok huge fat man vs strong man glad times changed terrible main event like every match luger terrible matches card razor ramon vs ted dibiase steiner brothers vs heavenly bodies shawn michaels vs curt hening event shawn named big monster body guard diesel irs vs 123 kid bret hart first takes doink takes jerry lawler stuff harts lawler always interesting ludvig borga destroyed marty jannetty undertaker took giant gonzalez another terrible match smoking gunns tatanka took bam bam bigelow headshrinkers yokozuna defended world title lex luger match boring terrible ending however deserves 810",
         "positive",
         "['really', 'liked', 'summerslam', 'due', 'look', 'arena', 'curtains', 'look', 'overall', 'interesting', 'reason', 'anyways', 'could', 'one', 'best', 'summerslams', 'ever', 'wwf', 'lex', 'luger', 'main', 'event', 'yokozuna', 'time', 'ok', 'huge', 'fat', 'man', 'vs', 'strong', 'man', 'glad', 'times', 'changed', 'terrible', 'main', 'event', 'like', 'every', 'match', 'luger', 'terrible', 'matches', 'card', 'razor', 'ramon', 'vs', 'ted', 'dibiase', 'steiner', 'brothers', 'vs', 'heavenly', 'bodies', 'shawn', 'michaels', 'vs', 'curt', 'hening', 'event', 'shawn', 'named', 'big', 'monster', 'body', 'guard', 'diesel', 'irs', 'vs', '123', 'kid', 'bret', 'hart', 'first', 'takes', 'doink', 'takes', 'jerry', 'lawler', 'stuff', 'harts', 'lawler', 'always', 'interesting', 'ludvig', 'borga', 'destroyed', 'marty', 'jannetty', 'undertaker', 'took', 'giant', 'gonzalez', 'another', 'terrible', 'match', 'smoking', 'gunns', 'tatanka', 'took', 'bam', 'bam', 'bigelow', 'headshrinkers', 'yokozuna', 'defended', 'world', 'title', 'lex', 'luger', 'match', 'boring', 'terrible', 'ending', 'however', 'deserves', '810']",
         "117",
         "752"
        ],
        [
         "1",
         "many television shows appeal quite many different kinds fans like farscape doesi know youngsters 3040 years oldfans male female many different countries think adore tv miniseries elements found almost every show tv character driven drama could australian soap opera yet episode science fact  fiction would give even hardiest trekkie run money brainbender stakes wormhole theory time travel true equational formmagnificent embraces cultures map possibilities endless multiple stars therefore thousands planets choose from broad scope would expected nothing would able keep illusion long farscape really comes elementit succeeds others failed especially likes star trek a universe practically zero kaos element ran ideas pretty quickly  kept rehashing them course 4 seasons manage keep audiences attention using good continuity constant character evolution multiple threads every episode unique personal touches camera specific certain character groups within whole structure allows extremely large area subject matter loyalties forged broken many ways many many issues happened see pilot premiere passing keep tuning see crichton would ever get girl seeing television delighted see available dvd  admit thing kept sane whilst 12 hour night shift developed chronic insomniafarscape thing get extremely long nights favour watch pilot see mean farscape comet",
         "positive",
         "['many', 'television', 'shows', 'appeal', 'quite', 'many', 'different', 'kinds', 'fans', 'like', 'farscape', 'doesi', 'know', 'youngsters', '3040', 'years', 'oldfans', 'male', 'female', 'many', 'different', 'countries', 'think', 'adore', 'tv', 'miniseries', 'elements', 'found', 'almost', 'every', 'show', 'tv', 'character', 'driven', 'drama', 'could', 'australian', 'soap', 'opera', 'yet', 'episode', 'science', 'fact', 'fiction', 'would', 'give', 'even', 'hardiest', 'trekkie', 'run', 'money', 'brainbender', 'stakes', 'wormhole', 'theory', 'time', 'travel', 'true', 'equational', 'formmagnificent', 'embraces', 'cultures', 'map', 'possibilities', 'endless', 'multiple', 'stars', 'therefore', 'thousands', 'planets', 'choose', 'from', 'broad', 'scope', 'would', 'expected', 'nothing', 'would', 'able', 'keep', 'illusion', 'long', 'farscape', 'really', 'comes', 'elementit', 'succeeds', 'others', 'failed', 'especially', 'likes', 'star', 'trek', 'a', 'universe', 'practically', 'zero', 'kaos', 'element', 'ran', 'ideas', 'pretty', 'quickly', 'kept', 'rehashing', 'them', 'course', '4', 'seasons', 'manage', 'keep', 'audiences', 'attention', 'using', 'good', 'continuity', 'constant', 'character', 'evolution', 'multiple', 'threads', 'every', 'episode', 'unique', 'personal', 'touches', 'camera', 'specific', 'certain', 'character', 'groups', 'within', 'whole', 'structure', 'allows', 'extremely', 'large', 'area', 'subject', 'matter', 'loyalties', 'forged', 'broken', 'many', 'ways', 'many', 'many', 'issues', 'happened', 'see', 'pilot', 'premiere', 'passing', 'keep', 'tuning', 'see', 'crichton', 'would', 'ever', 'get', 'girl', 'seeing', 'television', 'delighted', 'see', 'available', 'dvd', 'admit', 'thing', 'kept', 'sane', 'whilst', '12', 'hour', 'night', 'shift', 'developed', 'chronic', 'insomniafarscape', 'thing', 'get', 'extremely', 'long', 'nights', 'favour', 'watch', 'pilot', 'see', 'mean', 'farscape', 'comet']",
         "191",
         "1354"
        ],
        [
         "2",
         "film quickly gets major chase scene ever increasing destruction first really bad thing guy hijacking steven seagal would beaten pulp seagals driving probably would ended whole premise movie seems like decided make kinds changes movie plot plan enjoy action expect coherent plot turn sense logic may have reduce chance getting headache give hope steven seagal trying move back towards type characters portrayed popular movies",
         "negative",
         "['film', 'quickly', 'gets', 'major', 'chase', 'scene', 'ever', 'increasing', 'destruction', 'first', 'really', 'bad', 'thing', 'guy', 'hijacking', 'steven', 'seagal', 'would', 'beaten', 'pulp', 'seagals', 'driving', 'probably', 'would', 'ended', 'whole', 'premise', 'movie', 'seems', 'like', 'decided', 'make', 'kinds', 'changes', 'movie', 'plot', 'plan', 'enjoy', 'action', 'expect', 'coherent', 'plot', 'turn', 'sense', 'logic', 'may', 'have', 'reduce', 'chance', 'getting', 'headache', 'give', 'hope', 'steven', 'seagal', 'trying', 'move', 'back', 'towards', 'type', 'characters', 'portrayed', 'popular', 'movies']",
         "64",
         "424"
        ],
        [
         "3",
         "jane austen would definitely approve one gwyneth paltrow awesome job capturing attitude emma funny without excessively silly yet elegant puts convincing british accent not british myself maybe best judge fooled meshe also excellent sliding doorsi sometimes forget american  also brilliant jeremy northam sophie thompson phyllida law emma thompsons sister mother bates women nearly steal showand ms law even lines highly recommended",
         "positive",
         "['jane', 'austen', 'would', 'definitely', 'approve', 'one', 'gwyneth', 'paltrow', 'awesome', 'job', 'capturing', 'attitude', 'emma', 'funny', 'without', 'excessively', 'silly', 'yet', 'elegant', 'puts', 'convincing', 'british', 'accent', 'not', 'british', 'myself', 'maybe', 'best', 'judge', 'fooled', 'meshe', 'also', 'excellent', 'sliding', 'doorsi', 'sometimes', 'forget', 'american', 'also', 'brilliant', 'jeremy', 'northam', 'sophie', 'thompson', 'phyllida', 'law', 'emma', 'thompsons', 'sister', 'mother', 'bates', 'women', 'nearly', 'steal', 'showand', 'ms', 'law', 'even', 'lines', 'highly', 'recommended']",
         "61",
         "431"
        ],
        [
         "4",
         "expectations somewhat high went see movie thought steve carell could wrong coming great movies like anchorman 40 yearold virgin little miss sunshine boy wrong start right movie certain points steve carell allowed steve carell handful moments film made laugh due almost entirely given wiggleroom thing undoubtedly talented individual shame signed turned be opinion total trainwreck way discuss went horrifyingly wrong film begins dan burns widower three girls considered nationally syndicated advice column prepares girls family reunion extended relatives gather time other family high atop list things make awful movie family behaves like this almost transported pleasantville leave beaver caricature think family 7 reaches point become obnoxious simply frustrating touch football crossword puzzle competitions family bowling talent shows actual people behave almost sickening another big flaw woman carell supposed falling for observing first scene steve carell like watching stroke victim trying rehabilitated imagine supposed unique original woman comes mildly retarded makes think movie taking place another planet left theater wondering saw thinking further think much",
         "negative",
         "['expectations', 'somewhat', 'high', 'went', 'see', 'movie', 'thought', 'steve', 'carell', 'could', 'wrong', 'coming', 'great', 'movies', 'like', 'anchorman', '40', 'yearold', 'virgin', 'little', 'miss', 'sunshine', 'boy', 'wrong', 'start', 'right', 'movie', 'certain', 'points', 'steve', 'carell', 'allowed', 'steve', 'carell', 'handful', 'moments', 'film', 'made', 'laugh', 'due', 'almost', 'entirely', 'given', 'wiggleroom', 'thing', 'undoubtedly', 'talented', 'individual', 'shame', 'signed', 'turned', 'be', 'opinion', 'total', 'trainwreck', 'way', 'discuss', 'went', 'horrifyingly', 'wrong', 'film', 'begins', 'dan', 'burns', 'widower', 'three', 'girls', 'considered', 'nationally', 'syndicated', 'advice', 'column', 'prepares', 'girls', 'family', 'reunion', 'extended', 'relatives', 'gather', 'time', 'other', 'family', 'high', 'atop', 'list', 'things', 'make', 'awful', 'movie', 'family', 'behaves', 'like', 'this', 'almost', 'transported', 'pleasantville', 'leave', 'beaver', 'caricature', 'think', 'family', '7', 'reaches', 'point', 'become', 'obnoxious', 'simply', 'frustrating', 'touch', 'football', 'crossword', 'puzzle', 'competitions', 'family', 'bowling', 'talent', 'shows', 'actual', 'people', 'behave', 'almost', 'sickening', 'another', 'big', 'flaw', 'woman', 'carell', 'supposed', 'falling', 'for', 'observing', 'first', 'scene', 'steve', 'carell', 'like', 'watching', 'stroke', 'victim', 'trying', 'rehabilitated', 'imagine', 'supposed', 'unique', 'original', 'woman', 'comes', 'mildly', 'retarded', 'makes', 'think', 'movie', 'taking', 'place', 'another', 'planet', 'left', 'theater', 'wondering', 'saw', 'thinking', 'further', 'think', 'much']",
         "164",
         "1173"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>_words</th>\n",
       "      <th>_word_count</th>\n",
       "      <th>_char_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>really liked summerslam due look arena curtain...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[really, liked, summerslam, due, look, arena, ...</td>\n",
       "      <td>117</td>\n",
       "      <td>752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>many television shows appeal quite many differ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[many, television, shows, appeal, quite, many,...</td>\n",
       "      <td>191</td>\n",
       "      <td>1354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>film quickly gets major chase scene ever incre...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[film, quickly, gets, major, chase, scene, eve...</td>\n",
       "      <td>64</td>\n",
       "      <td>424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jane austen would definitely approve one gwyne...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[jane, austen, would, definitely, approve, one...</td>\n",
       "      <td>61</td>\n",
       "      <td>431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>expectations somewhat high went see movie thou...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[expectations, somewhat, high, went, see, movi...</td>\n",
       "      <td>164</td>\n",
       "      <td>1173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  really liked summerslam due look arena curtain...  positive   \n",
       "1  many television shows appeal quite many differ...  positive   \n",
       "2  film quickly gets major chase scene ever incre...  negative   \n",
       "3  jane austen would definitely approve one gwyne...  positive   \n",
       "4  expectations somewhat high went see movie thou...  negative   \n",
       "\n",
       "                                              _words  _word_count  _char_len  \n",
       "0  [really, liked, summerslam, due, look, arena, ...          117        752  \n",
       "1  [many, television, shows, appeal, quite, many,...          191       1354  \n",
       "2  [film, quickly, gets, major, chase, scene, eve...           64        424  \n",
       "3  [jane, austen, would, definitely, approve, one...           61        431  \n",
       "4  [expectations, somewhat, high, went, see, movi...          164       1173  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Word-based tokenization (for stats, ngrams, RNNs)\n",
    "preprocessor_word = TextPreprocessor(\n",
    "    DATA_PATH,\n",
    "    sample_size=SAMPLE_SIZE,\n",
    "    tokenizer_type=\"word\"\n",
    ")\n",
    "\n",
    "df_word = preprocessor_word.load_data(remove_stopwords=True) # true for better stats\n",
    "train_df, val_df, test_df = preprocessor_word.get_splits(train_index=train_idx, val_index=val_idx, test_index=test_idx)\n",
    "\n",
    "print(\"\\nDataset loaded successfully!\")\n",
    "display(df_word.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Statistics & Visualization\n",
    "\n",
    "Compute statistics and visualize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics\n",
    "stats_computer = IMDBDataStats(df_word)\n",
    "stats = stats_computer.get_full_stats()\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nClass Distribution: {stats['class_distribution']}\")\n",
    "print(f\"\\nVocabulary Size: {stats['vocabulary_size']}\")\n",
    "print(f\"\\nAverage Word Count: {stats['average_word_count']}\")\n",
    "print(f\"\\nMost Frequent Words (Overall):\")\n",
    "for word, count in stats['most_frequent_words']['overall']:\n",
    "    print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize word clouds\n",
    "stats_computer.visualize_word_clouds(save_path=\"out/wordclouds.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate scattertext visualization (interactive HTML)\n",
    "stats_computer.visualize_scattertext(output_html=\"out/scattertext.html\")\n",
    "print(\"Open out/scattertext.html in your browser to view the interactive visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. N-gram Language Models\n",
    "\n",
    "Train bigram and trigram models, generate text, and compute perplexity.\n",
    "\n",
    "### N-gram Pipeline\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Tokenized Text] --> B[Build N-gram Counts]\n",
    "    B --> C{N-gram Type}\n",
    "    C -->|n=2| D[Bigram Model]\n",
    "    C -->|n=3| E[Trigram Model]\n",
    "    \n",
    "    D --> F[Laplace Smoothing]\n",
    "    E --> F\n",
    "    \n",
    "    F --> G[Probability Estimation]\n",
    "    G --> H[Text Generation]\n",
    "    G --> I[Perplexity Computation]\n",
    "    \n",
    "    H --> J[Sample Sentences]\n",
    "    I --> K[Model Quality Metric]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style F fill:#fff4e1\n",
    "    style J fill:#e8f5e9\n",
    "    style K fill:#e8f5e9\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new dataframe loaded with stopwords\n",
    "\n",
    "train_df = preprocessor_word.load_data(remove_stopwords=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Bigram Model\n",
    "print(\"Training Bigram Model...\")\n",
    "bigram_model = NGramModel(n=2, laplace_smoothing=True)\n",
    "bigram_model.train(train_df[\"_words\"])\n",
    "\n",
    "print(\"\\n--- Bigram Generation Examples ---\")\n",
    "for i in range(3):\n",
    "    print(f\"{i+1}. {bigram_model.generate_sentence(max_length=20)}\")\n",
    "\n",
    "print(\"\\nComputing perplexity...\")\n",
    "bigram_perplexity = bigram_model.compute_perplexity(test_df[\"_words\"])\n",
    "print(f\"Bigram Test Perplexity: {bigram_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Trigram Model\n",
    "print(\"Training Trigram Model...\")\n",
    "trigram_model = NGramModel(n=3, laplace_smoothing=True)\n",
    "trigram_model.train(train_df[\"_words\"])\n",
    "\n",
    "print(\"\\n--- Trigram Generation Examples ---\")\n",
    "for i in range(3):\n",
    "    print(f\"{i+1}. {trigram_model.generate_sentence(max_length=20)}\")\n",
    "\n",
    "print(\"\\nComputing perplexity...\")\n",
    "trigram_perplexity = trigram_model.compute_perplexity(test_df[\"_words\"])\n",
    "print(f\"Trigram Test Perplexity: {trigram_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. RNN Models (LSTM/GRU)\n",
    "\n",
    "Train recurrent neural networks for language modeling and text generation.\n",
    "\n",
    "### RNN Training Pipeline\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Word Tokens] --> B[Build Vocabulary]\n",
    "    B --> C[Token to ID Mapping]\n",
    "    C --> D[Create DataLoader]\n",
    "    D --> E[RNN Model]\n",
    "    \n",
    "    E --> F{RNN Type}\n",
    "    F -->|LSTM| G[LSTM Layers]\n",
    "    F -->|GRU| H[GRU Layers]\n",
    "    \n",
    "    G --> I[Embedding Layer]\n",
    "    H --> I\n",
    "    I --> J[Hidden States]\n",
    "    J --> K[Output Layer]\n",
    "    \n",
    "    K --> L[Cross-Entropy Loss]\n",
    "    L --> M[Backpropagation]\n",
    "    M --> N{Converged?}\n",
    "    N -->|No| E\n",
    "    N -->|Yes| O[Trained Model]\n",
    "    \n",
    "    O --> P[Text Generation]\n",
    "    O --> Q[Perplexity Evaluation]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style E fill:#fff4e1\n",
    "    style O fill:#e8f5e9\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "data_module = RNNDataModule()\n",
    "data_module.build_vocab(train_df[\"_words\"].tolist(), min_freq=5) # train_df with stopwords\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = data_module.get_dataloader(\n",
    "    df=train_df,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    max_seq_length=128,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "val_loader = data_module.get_dataloader(\n",
    "    df=val_df,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    max_seq_length=128,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "test_loader = data_module.get_dataloader(\n",
    "    df=test_df,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    max_seq_length=128,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "print(f\"Vocabulary size: {len(data_module.vocab)}\")\n",
    "print(f\"Train batches: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM Model\n",
    "print(\"Training LSTM Model...\")\n",
    "\n",
    "lstm_model = RNN(\n",
    "    vocab_size=len(data_module.vocab),\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=256,\n",
    "    rnn_type='LSTM',\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0)  # padding_idx=0\n",
    "optimizer = torch.optim.AdamW(lstm_model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2)\n",
    "\n",
    "lstm_model.train_loop(\n",
    "    train_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    num_epochs=3,  # Reduce for demo\n",
    "    accumulation_steps=4\n",
    ")\n",
    "\n",
    "# Save model\n",
    "lstm_model.save_model(\"out/lstm_imdb_model.pth\")\n",
    "print(\"Model saved to out/lstm_imdb_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with LSTM\n",
    "print(\"\\n--- LSTM Generation Examples ---\")\n",
    "start_token_id = data_module.vocab.get(\"<s>\", 2)\n",
    "\n",
    "for i in range(3):\n",
    "    generated_sequence = lstm_model.generate(\n",
    "        start_token=start_token_id,\n",
    "        max_length=30,\n",
    "        temperature=0.8\n",
    "    )\n",
    "    generated_text = data_module.decode_sequence(generated_sequence)\n",
    "    print(f\"{i+1}. {generated_text}\")\n",
    "\n",
    "# Compute perplexity\n",
    "lstm_perplexity = lstm_model.compute_perplexity(test_loader)\n",
    "print(f\"\\nLSTM Test Perplexity: {lstm_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GRU Model (optional)\n",
    "print(\"Training GRU Model...\")\n",
    "\n",
    "gru_model = RNN(\n",
    "    vocab_size=len(data_module.vocab),\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=256,\n",
    "    rnn_type='GRU',\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(gru_model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2)\n",
    "\n",
    "gru_model.train_loop(\n",
    "    train_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    num_epochs=3,\n",
    "    accumulation_steps=4\n",
    ")\n",
    "\n",
    "gru_model.save_model(\"out/gru_imdb_model.pth\")\n",
    "print(\"Model saved to out/gru_imdb_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with GRU\n",
    "print(\"\\n--- GRU Generation Examples ---\")\n",
    "\n",
    "for i in range(3):\n",
    "    generated_sequence = gru_model.generate(\n",
    "        start_token=start_token_id,\n",
    "        max_length=30,\n",
    "        temperature=0.8\n",
    "    )\n",
    "    generated_text = data_module.decode_sequence(generated_sequence)\n",
    "    print(f\"{i+1}. {generated_text}\")\n",
    "\n",
    "# Compute perplexity\n",
    "gru_perplexity = gru_model.compute_perplexity(test_loader)\n",
    "print(f\"\\nGRU Test Perplexity: {gru_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Transformer Models (Fine-tuning)\n",
    "\n",
    "Fine-tune pre-trained transformers (ALBERT) on sentiment classification.\n",
    "\n",
    "### Transformer Fine-tuning Pipeline\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[IMDB Dataset] --> B[Subword Tokenization]\n",
    "    B --> C{Tokenizer Type}\n",
    "    C -->|WordPiece| D[BERT/MiniLM Tokenizer]\n",
    "    C -->|BPE| E[RoBERTa Tokenizer]\n",
    "    C -->|Unigram| F[ALBERT Tokenizer]\n",
    "    \n",
    "    D --> G[Token IDs + Attention Masks]\n",
    "    E --> G\n",
    "    F --> G\n",
    "    \n",
    "    G --> H[Pre-trained Model]\n",
    "    H --> I[Add Classification Head]\n",
    "    I --> J[Fine-tuning]\n",
    "    \n",
    "    J --> K[Training Loop]\n",
    "    K --> L[Gradient Accumulation]\n",
    "    L --> M[AdamW Optimizer]\n",
    "    M --> N{Epoch Complete?}\n",
    "    N -->|No| K\n",
    "    N -->|Yes| O[Save Model]\n",
    "    \n",
    "    O --> P[Evaluation]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style C fill:#fff4e1\n",
    "    style O fill:#e8f5e9\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Transformer...\n",
      "Using device: mps\n",
      "\n",
      "Vocabulary size for unigram tokenizer: 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient checkpointing not supported for this model\n",
      "Loading data from dataset/imdb-dataset.csv...\n",
      "Loaded 1000 reviews (positive: 476, negative: 524)\n",
      "Dataset: 800 samples, avg_len=223.6, max_len=320\n",
      "Dataset: 100 samples, avg_len=232.3, max_len=320\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 1:14:25, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 100 samples, avg_len=222.7, max_len=320\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAM Test results: {'eval_loss': 0.5329983234405518, 'eval_runtime': 10.4234, 'eval_samples_per_second': 9.594, 'eval_steps_per_second': 0.192, 'epoch': 0.96}\n",
      "\n",
      "Transformer training complete!\n",
      "Model saved to out/minilm_imdb_model\n",
      "{'unigram': {'trainer': <transformers.trainer.Trainer object at 0x31a758880>, 'model': AlbertForSequenceClassification(\n",
      "  (albert): AlbertModel(\n",
      "    (embeddings): AlbertEmbeddings(\n",
      "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 128)\n",
      "      (token_type_embeddings): Embedding(2, 128)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (encoder): AlbertTransformer(\n",
      "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
      "      (albert_layer_groups): ModuleList(\n",
      "        (0): AlbertLayerGroup(\n",
      "          (albert_layers): ModuleList(\n",
      "            (0): AlbertLayer(\n",
      "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): AlbertSdpaAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (attention_dropout): Dropout(p=0, inplace=False)\n",
      "                (output_dropout): Dropout(p=0, inplace=False)\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              )\n",
      "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (activation): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (pooler_activation): Tanh()\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "), 'tokenizer': AlbertTokenizerFast(name_or_path='albert-base-v2', vocab_size=30000, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '<unk>', 'sep_token': '[SEP]', 'pad_token': '<pad>', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "), 'metrics': {'eval_loss': 0.5329983234405518, 'eval_runtime': 10.4234, 'eval_samples_per_second': 9.594, 'eval_steps_per_second': 0.192, 'epoch': 0.96}}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Fine-tuning Transformer...\")\n",
    "\n",
    "transformer_outputs = finetune_minilm(\n",
    "    data_path=DATA_PATH,\n",
    "    sample_size=SAMPLE_SIZE,\n",
    "    output_dir=\"out/minilm_imdb_model\",\n",
    "    tokenization=\"unigram\",  # Options: \"wordpiece\", \"bpe\", \"unigram\", \"all\"\n",
    "    epochs=1,\n",
    "    train_index=train_idx,\n",
    "    val_index=val_idx,\n",
    "    test_index=test_idx\n",
    ")\n",
    "\n",
    "print(\"\\nTransformer training complete!\")\n",
    "print(f\"Model saved to out/minilm_imdb_model\")\n",
    "print(transformer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Transformer Evaluation\n",
    "\n",
    "Evaluate the fine-tuned transformer with:\n",
    "- Standard metrics (Accuracy, F1, Confusion Matrix, Perplexity)\n",
    "- LLM-as-a-judge (optional, requires API key)\n",
    "\n",
    "### Three-Part Evaluation Pipeline\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Trained Model] --> B[Random 100 Test Samples]\n",
    "    \n",
    "    B --> C[Part 1: Metric-based]\n",
    "    B --> D[Part 2: Human Evaluation]\n",
    "    B --> E[Part 3: LLM-as-a-Judge]\n",
    "    \n",
    "    C --> F[Accuracy]\n",
    "    C --> G[F1 Score]\n",
    "    C --> H[Perplexity]\n",
    "    C --> I[Confusion Matrix]\n",
    "    \n",
    "    D --> J[CSV Export]\n",
    "    J --> K[Manual Annotation]\n",
    "    K --> L[Human Ratings]\n",
    "    \n",
    "    E --> M[Llama 3.1 8B]\n",
    "    M --> N[LLM Judgments]\n",
    "    \n",
    "    F --> O[Agreement Analysis]\n",
    "    G --> O\n",
    "    H --> O\n",
    "    I --> O\n",
    "    L --> O\n",
    "    N --> O\n",
    "    \n",
    "    O --> P[Compare All Evaluations]\n",
    "    P --> Q[Disagreement Examples]\n",
    "    P --> R[Final Report]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style C fill:#fff4e1\n",
    "    style D fill:#fff4e1\n",
    "    style E fill:#fff4e1\n",
    "    style R fill:#e8f5e9\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[503 182 622 117 333 791 314 586 637 927 451 982 287 548 137 141  15 634\n",
      " 609 675 526 300 752 286  10 212 275  57 195 648 102  69 611 821 967 317\n",
      " 534 948 437 553 973 432 932 809 689  41 720 970  42  49 518 167 535 528\n",
      "  63 910 366 706 651 272 313 621 943 617 908 628 669 641 702 490 828 290\n",
      " 502 296 832 583 749   1 779 177 303 961 547 837 736 529 974 372 939 500\n",
      " 543 672  76 491 735 555  34 113 410 687]\n",
      "\n",
      "Evaluating Transformer model (wordpiece)...\n",
      "Using device: mps\n",
      "\n",
      "Loading model...\n",
      "Transformer vocabulary size: 30522\n",
      "\n",
      "Loading data...\n",
      "Loading data from dataset/imdb-dataset.csv...\n",
      "Loaded 1000 reviews (positive: 476, negative: 524)\n",
      "Dataset: 800 samples, avg_len=221.2, max_len=320\n",
      "Dataset: 100 samples, avg_len=230.0, max_len=320\n",
      "Dataset: 100 samples, avg_len=220.8, max_len=320\n",
      "\n",
      "============================================================\n",
      "PART 1: METRIC-BASED EVALUATION (Full Splits)\n",
      "============================================================\n",
      "\n",
      "Train Metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.6488\n",
      "F1 Score: 0.7347\n",
      "Perplexity: 1.9943\n",
      "Confusion Matrix:\n",
      "[[130 278]\n",
      " [  3 389]]\n",
      "\n",
      "Validation Metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.5900\n",
      "F1 Score: 0.6720\n",
      "Perplexity: 1.9964\n",
      "Confusion Matrix:\n",
      "[[17 41]\n",
      " [ 0 42]]\n",
      "\n",
      "Test Metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.5500\n",
      "F1 Score: 0.6457\n",
      "Perplexity: 1.9960\n",
      "Confusion Matrix:\n",
      "[[14 44]\n",
      " [ 1 41]]\n",
      "\n",
      "============================================================\n",
      "PART 2: SUBSAMPLE (100 Test Instances)\n",
      "============================================================\n",
      "\n",
      "Generating predictions for 100 samples...\n",
      "\n",
      "Subsample Metrics (using gold labels):\n",
      "  Accuracy: 0.6100\n",
      "  F1 Score: 0.7023\n",
      "\n",
      "============================================================\n",
      "PART 3: HUMAN EVALUATION (Manual Rating)\n",
      "============================================================\n",
      "\n",
      "✓ Saved evaluation data to: out/evaluation_results.csv\n",
      "\n",
      "Instructions for Human Evaluation:\n",
      "  1. Open the CSV file in a spreadsheet editor\n",
      "  2. Fill in the 'human_rating' column with 'positive' or 'negative'\n",
      "  3. You and your teammates should rate each review independently\n",
      "  4. Save the file and re-run this script to see agreement analysis\n",
      "\n",
      "Note: Gold labels are provided for reference, but rate based on your judgment!\n",
      "\n",
      "============================================================\n",
      "PART 4: LLM-AS-A-JUDGE EVALUATION (Llama 3.2 3B)\n",
      "============================================================\n",
      "\n",
      "==================================================\n",
      "LLM JUDGE EVALUATION (Gemini 3 Flash Preview)\n",
      "==================================================\n",
      "Evaluating 100 samples...\n",
      "This may take a few minutes...\n",
      "\n",
      "Processing 1/100... ✓ negative\n",
      "Processing 2/100... ✓ negative\n",
      "Processing 3/100... ✓ negative\n",
      "Processing 4/100... ✓ negative\n",
      "Processing 5/100... ✓ positive\n",
      "Processing 6/100... ✓ negative\n",
      "Processing 7/100... ✓ negative\n",
      "Processing 8/100... ✓ positive\n",
      "Processing 9/100... ✓ negative\n",
      "Processing 10/100... ✓ negative\n",
      "Processing 11/100... ✓ positive\n",
      "Processing 12/100... ✓ negative\n",
      "Processing 13/100... ✓ positive\n",
      "Processing 14/100... ✓ negative\n",
      "Processing 15/100... ✓ negative\n",
      "Processing 16/100... ✓ negative\n",
      "Processing 17/100... ✓ negative\n",
      "Processing 18/100... ✓ positive\n",
      "Processing 19/100... ✓ positive\n",
      "Processing 20/100... ✓ negative\n",
      "Processing 21/100... ✓ negative\n",
      "Processing 22/100... ✓ negative\n",
      "Processing 23/100... ✓ negative\n",
      "Processing 24/100... ✓ positive\n",
      "Processing 25/100... ✓ negative\n",
      "Processing 26/100... ✓ negative\n",
      "Processing 27/100... ✓ positive\n",
      "Processing 28/100... ✓ negative\n",
      "Processing 29/100... ✓ negative\n",
      "Processing 30/100... ✓ positive\n",
      "Processing 31/100... ✓ positive\n",
      "Processing 32/100... ✓ negative\n",
      "Processing 33/100... ✓ positive\n",
      "Processing 34/100... ✓ positive\n",
      "Processing 35/100... ✓ positive\n",
      "Processing 36/100... ✓ positive\n",
      "Processing 37/100... ✓ positive\n",
      "Processing 38/100... ✓ positive\n",
      "Processing 39/100... ✓ negative\n",
      "Processing 40/100... ✓ negative\n",
      "Processing 41/100... ✓ negative\n",
      "Processing 42/100... ✓ negative\n",
      "Processing 43/100... ✓ negative\n",
      "Processing 44/100... ✓ negative\n",
      "Processing 45/100... ✓ positive\n",
      "Processing 46/100... ✓ positive\n",
      "Processing 47/100... ✓ negative\n",
      "Processing 48/100... ✓ negative\n",
      "Processing 49/100... ✓ negative\n",
      "Processing 50/100... ✓ negative\n",
      "Processing 51/100... ✓ positive\n",
      "Processing 52/100... ✓ negative\n",
      "Processing 53/100... ✓ negative\n",
      "Processing 54/100... ✓ positive\n",
      "Processing 55/100... ✓ negative\n",
      "Processing 56/100... ✓ positive\n",
      "Processing 57/100... ✓ negative\n",
      "Processing 58/100... ✓ positive\n",
      "Processing 59/100... ✓ positive\n",
      "Processing 60/100... ✓ positive\n",
      "Processing 61/100... ✓ negative\n",
      "Processing 62/100... ✓ positive\n",
      "Processing 63/100... ✓ positive\n",
      "Processing 64/100... ✓ negative\n",
      "Processing 65/100... ✓ positive\n",
      "Processing 66/100... ✓ positive\n",
      "Processing 67/100... ✓ positive\n",
      "Processing 68/100... ✓ positive\n",
      "Processing 69/100... ✓ negative\n",
      "Processing 70/100... ✓ negative\n",
      "Processing 71/100... ✓ negative\n",
      "Processing 72/100... ✓ positive\n",
      "Processing 73/100... ✓ positive\n",
      "Processing 74/100... ✓ negative\n",
      "Processing 75/100... ✓ positive\n",
      "Processing 76/100... ✓ negative\n",
      "Processing 77/100... ✓ positive\n",
      "Processing 78/100... ✓ positive\n",
      "Processing 79/100... ✓ negative\n",
      "Processing 80/100... ✓ negative\n",
      "Processing 81/100... ✓ positive\n",
      "Processing 82/100... ✓ positive\n",
      "Processing 83/100... ✓ positive\n",
      "Processing 84/100... ✓ positive\n",
      "Processing 85/100... ✓ negative\n",
      "Processing 86/100... ✓ negative\n",
      "Processing 87/100... ✓ negative\n",
      "Processing 88/100... ✓ negative\n",
      "Processing 89/100... ✓ positive\n",
      "Processing 90/100... ✓ negative\n",
      "Processing 91/100... ✓ negative\n",
      "Processing 92/100... ✓ negative\n",
      "Processing 93/100... ✓ positive\n",
      "Processing 94/100... ✓ positive\n",
      "Processing 95/100... ✓ positive\n",
      "Processing 96/100... ✓ negative\n",
      "Processing 97/100... ✓ negative\n",
      "Processing 98/100... ✓ positive\n",
      "Processing 99/100... ✓ negative\n",
      "Processing 100/100... ✓ negative\n",
      "\n",
      "✓ Updated out/evaluation_results.csv with LLM judgments\n",
      "\n",
      "============================================================\n",
      "PART 5: CROSS-EVALUATION AGREEMENT ANALYSIS\n",
      "============================================================\n",
      "\n",
      "==================================================\n",
      "AGREEMENT ANALYSIS\n",
      "==================================================\n",
      "\n",
      "Model vs Gold Labels:\n",
      "  Agreement: 61/100 (61.0%)\n",
      "\n",
      "LLM Judge vs Gold Labels:\n",
      "  Agreement: 96/100 (96.0%)\n",
      "\n",
      "Model vs LLM Judge:\n",
      "  Agreement: 59/100 (59.0%)\n",
      "\n",
      "==================================================\n",
      "DISAGREEMENT EXAMPLES\n",
      "==================================================\n",
      "\n",
      "Both Model and LLM disagree with gold (1 cases):\n",
      "\n",
      "  Review: I think that you can not imagine how these people really work...!! Before I came to the studios to w...\n",
      "  Gold: negative, Model: positive, LLM: positive\n",
      "\n",
      "Model and LLM agree but differ from gold (1 cases):\n",
      "\n",
      "  Review: I think that you can not imagine how these people really work...!! Before I came to the studios to w...\n",
      "  Gold: negative, Model+LLM: positive\n",
      "\n",
      "============================================================\n",
      "EVALUATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Results saved to: out/evaluation_results.csv\n",
      "\n",
      "Next steps:\n",
      "  - Fill in human ratings in the CSV file\n",
      "  - Re-run this script to see complete agreement analysis\n"
     ]
    }
   ],
   "source": [
    "# Set Google Gemini API key (optional, for LLM judge)\n",
    "import os\n",
    "# Run evaluation\n",
    "# create subsample from test set\n",
    "print(selected_test_idx)\n",
    "\n",
    "for tokenization in [\"wordpiece\"]:\n",
    "    model_dir = f\"out/minilm_imdb_model_{tokenization}\"\n",
    "    print(f\"\\nEvaluating Transformer model ({tokenization})...\")\n",
    "    evaluate_transformer(\n",
    "        model_dir=model_dir,\n",
    "        tokenizer_type=tokenization,\n",
    "        train_index=train_idx,\n",
    "        eval_index=val_idx,\n",
    "        test_index=selected_test_idx,\n",
    "        test_path=selected_test_path,\n",
    "        data_path=DATA_PATH,\n",
    "        sample_size=SAMPLE_SIZE,\n",
    "        skip_human_eval=False,  # Set to False to generate CSV for human annotation\n",
    "        skip_llm_judge=False,  # Set to False to use LLM judge\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Transformer model (unigram)...\n",
      "Using device: mps\n",
      "\n",
      "Loading model...\n",
      "Transformer vocabulary size: 30000\n",
      "\n",
      "Loading data...\n",
      "Loading data from dataset/imdb-dataset.csv...\n",
      "Loaded 1000 reviews (positive: 476, negative: 524)\n",
      "Dataset: 800 samples, avg_len=223.6, max_len=320\n",
      "Dataset: 100 samples, avg_len=232.3, max_len=320\n",
      "Dataset: 100 samples, avg_len=222.7, max_len=320\n",
      "\n",
      "============================================================\n",
      "PART 1: METRIC-BASED EVALUATION (Full Splits)\n",
      "============================================================\n",
      "\n",
      "Train Metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.8125\n",
      "F1 Score: 0.7869\n",
      "Perplexity: 1.7115\n",
      "Confusion Matrix:\n",
      "[[373  35]\n",
      " [115 277]]\n",
      "\n",
      "Validation Metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.8200\n",
      "F1 Score: 0.7568\n",
      "Perplexity: 1.7154\n",
      "Confusion Matrix:\n",
      "[[54  4]\n",
      " [14 28]]\n",
      "\n",
      "Test Metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.8400\n",
      "F1 Score: 0.7838\n",
      "Perplexity: 1.7040\n",
      "Confusion Matrix:\n",
      "[[55  3]\n",
      " [13 29]]\n",
      "\n",
      "============================================================\n",
      "PART 2: SUBSAMPLE (100 Test Instances)\n",
      "============================================================\n",
      "\n",
      "Generating predictions for 100 samples...\n",
      "\n",
      "Subsample Metrics (using gold labels):\n",
      "  Accuracy: 0.9000\n",
      "  F1 Score: 0.8936\n",
      "\n",
      "============================================================\n",
      "PART 3: HUMAN EVALUATION (Manual Rating)\n",
      "============================================================\n",
      "\n",
      "✓ Saved evaluation data to: out/evaluation_results.csv\n",
      "\n",
      "Instructions for Human Evaluation:\n",
      "  1. Open the CSV file in a spreadsheet editor\n",
      "  2. Fill in the 'human_rating' column with 'positive' or 'negative'\n",
      "  3. You and your teammates should rate each review independently\n",
      "  4. Save the file and re-run this script to see agreement analysis\n",
      "\n",
      "Note: Gold labels are provided for reference, but rate based on your judgment!\n",
      "\n",
      "============================================================\n",
      "PART 4: LLM-AS-A-JUDGE EVALUATION (Llama 3.2 3B)\n",
      "============================================================\n",
      "\n",
      "==================================================\n",
      "LLM JUDGE EVALUATION (Gemini 3 Flash Preview)\n",
      "==================================================\n",
      "Evaluating 100 samples...\n",
      "This may take a few minutes...\n",
      "\n",
      "Processing 1/100... ✓ negative\n",
      "Processing 2/100... ✓ negative\n",
      "Processing 3/100... ✓ negative\n",
      "Processing 4/100... ✓ negative\n",
      "Processing 5/100... ✓ positive\n",
      "Processing 6/100... ✓ negative\n",
      "Processing 7/100... ✓ negative\n",
      "Processing 8/100... ✓ positive\n",
      "Processing 9/100... ✓ negative\n",
      "Processing 10/100... ✓ negative\n",
      "Processing 11/100... ✓ positive\n",
      "Processing 12/100... ✓ negative\n",
      "Processing 13/100... ✓ positive\n",
      "Processing 14/100... ✓ negative\n",
      "Processing 15/100... ✓ negative\n",
      "Processing 16/100... ✓ negative\n",
      "Processing 17/100... ✓ negative\n",
      "Processing 18/100... ✓ positive\n",
      "Processing 19/100... ✓ positive\n",
      "Processing 20/100... ✓ negative\n",
      "Processing 21/100... ✓ negative\n",
      "Processing 22/100... ✓ negative\n",
      "Processing 23/100... ✓ negative\n",
      "Processing 24/100... ✓ positive\n",
      "Processing 25/100... ✓ negative\n",
      "Processing 26/100... ✓ negative\n",
      "Processing 27/100... ✓ positive\n",
      "Processing 28/100... ✓ negative\n",
      "Processing 29/100... ✓ negative\n",
      "Processing 30/100... ✓ positive\n",
      "Processing 31/100... ✓ positive\n",
      "Processing 32/100... ✓ negative\n",
      "Processing 33/100... ✓ positive\n",
      "Processing 34/100... ✓ positive\n",
      "Processing 35/100... ✓ positive\n",
      "Processing 36/100... ✓ positive\n",
      "Processing 37/100... ✓ positive\n",
      "Processing 38/100... ✓ positive\n",
      "Processing 39/100... ✓ negative\n",
      "Processing 40/100... ✓ negative\n",
      "Processing 41/100... ✓ negative\n",
      "Processing 42/100... ✓ negative\n",
      "Processing 43/100... ✓ negative\n",
      "Processing 44/100... ✓ negative\n",
      "Processing 45/100... ✓ positive\n",
      "Processing 46/100... ✓ positive\n",
      "Processing 47/100... ✓ negative\n",
      "Processing 48/100... ✓ negative\n",
      "Processing 49/100... ✓ negative\n",
      "Processing 50/100... ✓ negative\n",
      "Processing 51/100... ✓ positive\n",
      "Processing 52/100... ✓ negative\n",
      "Processing 53/100... ✓ negative\n",
      "Processing 54/100... ✓ positive\n",
      "Processing 55/100... ✓ negative\n",
      "Processing 56/100... ✓ positive\n",
      "Processing 57/100... ✓ negative\n",
      "Processing 58/100... ✓ positive\n",
      "Processing 59/100... ✓ positive\n",
      "Processing 60/100... ✓ positive\n",
      "Processing 61/100... ✓ negative\n",
      "Processing 62/100... ✓ positive\n",
      "Processing 63/100... ✓ positive\n",
      "Processing 64/100... ✓ negative\n",
      "Processing 65/100... ✓ positive\n",
      "Processing 66/100... ✓ positive\n",
      "Processing 67/100... ✓ positive\n",
      "Processing 68/100... ✓ positive\n",
      "Processing 69/100... ✓ negative\n",
      "Processing 70/100... ✓ negative\n",
      "Processing 71/100... ✓ negative\n",
      "Processing 72/100... ✓ positive\n",
      "Processing 73/100... ✓ positive\n",
      "Processing 74/100... ✓ negative\n",
      "Processing 75/100... ✓ positive\n",
      "Processing 76/100... ✓ negative\n",
      "Processing 77/100... ✓ positive\n",
      "Processing 78/100... ✓ positive\n",
      "Processing 79/100... ✓ negative\n",
      "Processing 80/100... ✓ negative\n",
      "Processing 81/100... ✓ positive\n",
      "Processing 82/100... ✓ positive\n",
      "Processing 83/100... ✓ positive\n",
      "Processing 84/100... ✓ positive\n",
      "Processing 85/100... ✓ negative\n",
      "Processing 86/100... ✓ negative\n",
      "Processing 87/100... ✓ negative\n",
      "Processing 88/100... ✓ negative\n",
      "Processing 89/100... ✓ positive\n",
      "Processing 90/100... ✓ negative\n",
      "Processing 91/100... ✓ negative\n",
      "Processing 92/100... ✓ negative\n",
      "Processing 93/100... ✓ positive\n",
      "Processing 94/100... ✓ positive\n",
      "Processing 95/100... ✓ positive\n",
      "Processing 96/100... ✓ negative\n",
      "Processing 97/100... ✓ negative\n",
      "Processing 98/100... ✓ positive\n",
      "Processing 99/100... ✓ negative\n",
      "Processing 100/100... ✓ negative\n",
      "\n",
      "✓ Updated out/evaluation_results.csv with LLM judgments\n",
      "\n",
      "============================================================\n",
      "PART 5: CROSS-EVALUATION AGREEMENT ANALYSIS\n",
      "============================================================\n",
      "\n",
      "==================================================\n",
      "AGREEMENT ANALYSIS\n",
      "==================================================\n",
      "\n",
      "Model vs Gold Labels:\n",
      "  Agreement: 90/100 (90.0%)\n",
      "\n",
      "LLM Judge vs Gold Labels:\n",
      "  Agreement: 96/100 (96.0%)\n",
      "\n",
      "Model vs LLM Judge:\n",
      "  Agreement: 88/100 (88.0%)\n",
      "\n",
      "==================================================\n",
      "DISAGREEMENT EXAMPLES\n",
      "==================================================\n",
      "\n",
      "Both Model and LLM disagree with gold (1 cases):\n",
      "\n",
      "  Review: Seven Ups has been compared to Bullitt for the chase scene, but does not come anywhere near matching...\n",
      "  Gold: positive, Model: negative, LLM: negative\n",
      "\n",
      "Model and LLM agree but differ from gold (1 cases):\n",
      "\n",
      "  Review: Seven Ups has been compared to Bullitt for the chase scene, but does not come anywhere near matching...\n",
      "  Gold: positive, Model+LLM: negative\n",
      "\n",
      "============================================================\n",
      "EVALUATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Results saved to: out/evaluation_results.csv\n",
      "\n",
      "Next steps:\n",
      "  - Fill in human ratings in the CSV file\n",
      "  - Re-run this script to see complete agreement analysis\n"
     ]
    }
   ],
   "source": [
    "for tokenization in [\"unigram\"]:\n",
    "    model_dir = f\"out/minilm_imdb_model\"\n",
    "    print(f\"\\nEvaluating Transformer model ({tokenization})...\")\n",
    "    evaluate_transformer(\n",
    "        model_dir=model_dir,\n",
    "        tokenizer_type=tokenization,\n",
    "        train_index=train_idx,\n",
    "        eval_index=val_idx,\n",
    "        test_index=selected_test_idx,\n",
    "        test_path=selected_test_path,\n",
    "        data_path=DATA_PATH,\n",
    "        sample_size=SAMPLE_SIZE,\n",
    "        skip_human_eval=False,  # Set to False to generate CSV for human annotation\n",
    "        skip_llm_judge=False,  # Set to False to use LLM judge\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Notes\n",
    "\n",
    "- **Adjust `SAMPLE_SIZE`** in the configuration cell to control dataset size\n",
    "- **Training times** vary by model complexity and hardware\n",
    "- **Transformer evaluation** supports LLM-as-a-judge with free OpenRouter API\n",
    "- All models are saved to disk and can be reloaded\n",
    "\n",
    "### File Outputs (saved in `out/` directory)\n",
    "- `out/wordclouds.png` - Word cloud visualizations\n",
    "- `out/scattertext.html` - Interactive term frequency visualization\n",
    "- `out/lstm_imdb_model.pth` - Trained LSTM model\n",
    "- `out/gru_imdb_model.pth` - Trained GRU model\n",
    "- `out/minilm_imdb_model/` - Fine-tuned transformer model\n",
    "- `out/evaluation_results.csv` - Evaluation results for manual annotation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
